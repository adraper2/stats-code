---
title: "MTH 341 - HW5"
author: "Aidan Draper"
date: "10/07/2018"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Homework 5

Page 178, #2, 3, 14, 23, 27, 34, 37, 57, 58

#### Question 2 (Expectations and variances):
In the Gregorian calendar, each year has either 365 days (a normal year) or 366 days (a leap year). A year is randomly chosen, with probability 3/4 of being a normal year and 1/4 of being a leap year. Find the mean and variance of the number of days in a chosen year.

\textit{We can use the standard formula of $E(X)=\sum x *p(x)$ for all values of x. In our case, there are only two that exist in the sample space. The variance of this can then be calculated using $E(X^2)-(E(X))^2$. $E(X^2)$ can be found by squaring the values first and then using the same formula for the expected mean.}
```{r q2}
vals <- c(365,366) # vector of values
probs <- c(.75,.25) # veector of probabilities
expected.mean <- sum(vals*probs)
expected.var <- sum(vals^2 * probs) - expected.mean^2
cat("The mean number of days in a chosen year is:", expected.mean, 
    "\nand the variance in the mean number of days in a chosen year is:", 
    expected.var)
```


#### Question 3 (Expectations and variances):
(a) A fair die is rolled. Find the expected value of that roll.

\textit{A fair die follows a discrete uniform distribution such that $X \sim \text{Dunif}(1,2,3,4,5,6)$. In order to calculate the expected value, we can quite easily calculate the sum of the probabilities multiplied by their values. This is done in R below.}
```{r q3a}
outcomes <- 1:6
prob <- 1/6
expected.die <- sum(outcomes*prob)
cat("The expected value of one die is", expected.die)
```

(b) Four fair die are rolled. Find the expected total of the rolls.

\textit{This problem is not much more complicated than the previous. In order to find $E(4X)$, it turns out that we can actually just use the theorem of linearity of expectation and pull the constant out so that we can multiply our expected value of one die by 4.}
```{r q3b}
expected.four.die <- 4 * expected.die
cat("The expected total value of four die is", expected.four.die)
```



#### Question 14 (Expectations and variances):
Let $X$ have PMF $$P(X=k)=cp^k/k \text{ for } k = 1,2,...,$$ where $p$ is a parameter with $0 < p < 1$ and $c$ is a normalizing constant. We have $c=-1/log(1-p)$ as seen in the Taylor series $$-log(1-p)=p+\frac{p^2}{2}+\frac{p^3}{3}+...$$ This distribution is called the $Logarithmic$ distribution (because of the log in the above Taylor series), and has often been used in ecology. Find the mean and variance of $X$.

\textit{To figure out the expected value, we must notice a trend in the infinite summation of the equation: $$\sum \limits_{k=1}^{\infty} c *\frac {p^k}{k} *k= \sum \limits_{k=1}^{\infty}cp^k$$This trend is called a geometric series where $a + ar + ar^2 + ar^3+..+ar^n$ converges on the formula $\frac{a}{1-r}$ for when the absolute value of r is less than 1. In our case, our $a$ value must be $c*p$ and our $r$ value is $p$ such that $$\sum \limits_{k=1}^{\infty} c *\frac {p^k}{k} = cp + cp^2 + cp^3 +...+cp^n=\frac{cp}{1-p} $$Now, it is apparent that our \textbf{expected value} is $$\frac{\frac{-1}{log(1-p)}*p}{1-p}$$ In order to find our expected variance, we need to calculate $Var(X)=E(X^2)-(E(X))^2$. Now, we must solve $E(X^2)$, which we can do using LOTUS, but it doesn't simplify as nicely: $$\sum \limits_{k=1}^{\infty} c *\frac {p^k}{k} *k^2= \sum \limits_{k=1}^{\infty}ckp^k$$That being said, we can use the formula for differntiating a Geometric expected value to solve for variance from Example 4.6.4, with some slight manipulation, to solve this problem. This formula was $$\sum \limits_{k=0}^{\infty}q^k=\frac{1}{1-q}$$, which differentiates to $$\sum \limits_{k=1}^{\infty}kq^{k-1}=\frac{1}{(1-q)^2}$$(Note that we use k=1, not k=0 because the resultant value of the formula for k=0 is just 0 when k is used. Therefore, we just start from k=1.) However, this forumla gives us $k(k-1)$ when we want $k^2$ so we will replenish the "supply of $q$'s" by muliplying both sides by q. This gives us $$\sum \limits_{k=0}^{\infty}kq^k=\frac{q}{(1-q)^2}$$This example is actually exactly the same as our problem except instead of $q$ we have $p$ and our resultant differentiated equation needs to include our constant, $c$, from before. Therefore, we have: $$c\sum \limits_{k=1}^{\infty}kp^k=c\frac{p}{(1-p)^2}$$Our \textbf{expected variance} is now: $$Var(X)=c\frac{p}{(1-p)^2}-(\frac{cp}{1-p})^2$$}


#### Question 23 (Expectations and variances):
Let $X\sim \text{Bin}(n,p)$ and $Y\sim \text{NBin}(r,p)$. Using a story about a sequence of a Bernoulli trial, prove that $P(X<r)=P(Y>n-r)$.

\textit{X represents the number of successful coin flips that land heads out of n flips for some arbitrary coin. Y represents the number of tails before r successful heads using the same coin. X<r therefore represents an event where there are less than r heads out of n flips. An important distinction now is that, for Y to exist, we might need to pass n trials if during that period we did not acheive the r number of heads that we desired. We could call this size s, which represents our total number of trials after n has been reached for our negative binomial. s-r will be greater than n-r in this case. It turns out that $P(X<r)=P(Y>n-r)$ because Y greater than "n trials minus r successes" is actually the same thing as the number of tails before X gets to the correct number of r heads. We just had to bound our negative binomial slightly differently with s because of how it operates in the story. Because our trials are either heads or tails, n-r represents our tails within n trials. However, we can actually operate a space past r successes in our negative binomial distribution. Nonetheless, they are equivalent because they have been normalized to equate to the same probability within two different sample spaces, n and s.}
```{r q23}

```

#### Question 27 (Expectations and variances):
(a) Use LOTUS to show that for $X\sim \text{Pois}(\lambda)$ and any function $g$, $$E(Xg(x))=\lambda E(g(X+1)).$$ This is called the \textit{Stein-Chen identity} for the Poisson.

\textit{These are essentially equivalent because of the convergence on Taylor Series and due to the fact that the first value, k=0, is actually meaningless in the expected value because the summation includes k in the formula. For example, our expacted value of $E(X)$ for k is equal to $$\sum \limits_{k=0}^{\infty}kg(k)\frac{\lambda^k}{k!}e^{-\lambda}$$For this to be setup correctly, we can throw out the first value of k=0 (its expected value just equals 0) such that our formula is now $$\sum \limits_{k=1}^{\infty}kg(k)\frac{\lambda^k}{k!}e^{-\lambda}$$From here, we can move our k into the denominator of the Poisson PMF so that we have factored out a k and also, factor out a lambda from our numerator so that the formula now is $$\lambda\sum \limits_{k=1}^{\infty}g(k)\frac{\lambda^{k-1}}{(k-1)!}e^{-\lambda}$$If we invent a new set j that equals k-1, we can plug in the values to show that $E(Xg(x))=\lambda E(g(X+1))$. $$\lambda\sum \limits_{j=0}^{\infty}g(j+1)\frac{\lambda^{j}}{(j)!}e^{-\lambda}$$which equals $\lambda E(g(X+1))$ because the summation after our constant, $\lambda$, is just the expected value of g(X+1).}

(b) Find the third moment $E(X^3)$ for $X \sim \text{Pois}(\lambda)$ by using the identity from (a) and a bit of algebra to reduce the calculation to the fact that $X$ has mean $\lambda$ and variance $\lambda$.

\textit{To find the variance of $X$, we need to use the formula $Var(X)=E(X^2)-(E(X))^2$. We need this in order to get $E(X^2)$, which equals $Var(X) + (E(X))^2$ with some slight formula manipulation. We can breakdown $E(X^3)=E(X)*E(X^2)$ so that it is easier to work with. This ends up being equivalent to the same formula we used $E(Xg(X))$ where $g(X)=X^2$. Now, we have $E(Xg(X))=\lambda E(g(X+1))$ from before. This becomes $E(Xg(X))=\lambda E((x+1)^2)$ when we plug in $g(X)$. Now using LOTUS, $$\lambda E((X+1)^2=\lambda \sum \limits_{k=0}^{\infty}k^2\frac{\lambda^k}{k!}e^{-\lambda}$$ and, from Example 4.7.2, we know that $E(X^2)$ equals $\lambda(\lambda +1)$, which means our final answer is $\lambda*\lambda(\lambda +1)=\lambda^3+\lambda^2$.}

#### Question 34 (Indicator r.v.s):
Each of $n \geq 2$ people puts his or her name on a slip of paper (no two have the same name). The slips of paper are shuffled in a hat, and then each person draws one (uniformly at random at each stage, without replacement). Find the average number of people who draw their own names.

\textit{Let X = 0,1,2,...,n-2,n because "everyone except for one person" cannot exist since the last person would have to draw their own name as well. We can setup each person as an indicator variable that yields either 1, a match, or a 0, no match, such that $X = I_1 + I_2+...+I_{n}$. $E(I_k)=1/n$ because of the fundamental bridge between probability and expectation for indicator random variables, which mimics Example 4.4.4 (matching continued) from the book. Therefore, our final answer is $E(X)=E(I_1)+E(I_2)+...+E(I_n)=n*1/n=1$ and we can expect 1 match regardless of the size, n.}


#### Question 37 (Indicator r.v.s):
You have a well-shuffled 52-card deck. On average, how many pairs of adjacent cards are there such that both cards are red?

\textit{There are 26-1 pairs because the first and last card cannot be a pair. The probability of any of these pairs simplifies to 26/52 (the first card) multiplied by the 25/51 (the second card) because we assume independence between trials, even though there definitely is not. This essentially also mimics our indicator random variable Example 4.4.4 (matching continued), except we have $\frac{26}{n}*\frac{25}{n-1}$ instead of $\frac{1}{n}$ and we multiply by 25 instead of $n$ because we care about the total, or count, of pairs of red cards. Nonetheless, now we can calculate the expected value as follows:}
```{r q37}
pairs <- rep(1,25)
prob <- ((26/52) * (25/51))
expected.pairs <- sum(pairs*prob)
cat("On average, we would expect",expected.pairs, "pairs.")
```

#### Question 57 (LOTUS):
For $X \sim \text{Pois}(\lambda)$, find $E(2^X)$, if it is finite.

\textit{For starters, a Poisson distribution is always going to be finite so this is not an issue. The expected value of $E(2^X)$ is $\sum \limits_{k=0}^{\infty}2^k\frac{\lambda}{k!}e^{-\lambda}$ due to LOTUS, which says that $E(g(x))=\sum \limits_{x}g(x)P(X=x)$. The PMF of X is from the Maclaurin Series, which says that $\sum \limits_{k=0}^{\infty}\frac{x^k}{k!}e^x$. $x$ is $\lambda$ in our case. Using this Taylor Series, we can simplify by pulling out $e^{-\lambda}$ of the summation because it is a constant. Now we have, $$E(2^X)=e^{-\lambda} \sum \limits_{k=0}^{\infty}2^k\frac{\lambda^k}{k!}=e^{-\lambda} \sum \limits_{k=0}^{\infty}\frac{(2\lambda)^k}{k!} = e^{-\lambda}e^{2\lambda}$$$e^{-\lambda}e^{2\lambda}$ is our final answer. We can simplify the summation because the sum of a valid PMF of a Poisson distribution expands into a Taylor Series and simplifies to e to the power of whatever is in the formula's numerator so long as those values were originally to the power of k. For example, $E(X)=\sum \limits_{k=0}^{\infty}\frac{\lambda^k}{k!}=e^\lambda$.}

#### Question 58 (LOTUS):
For $X \sim \text{Geom}(p)$, find $E(2^x)$ (if it is finite) and $E(2^{-X})$ (if it is finite). For each, make sure to clearly state what the values of $p$ are for which it is finite.

\textit{X = 0,1,2,... and we can compute $E(2^X)$ using LOTUS. $E(X) = \sum \limits_{k=0}^{\infty}kpq^k$, therfore $E(2^X)=\sum \limits_{k=0}^{\infty}2^kpq^k$. We can simplify the Taylor Series if we pull out the constant, $p$, such that $p\sum \limits_{k=0}^{\infty}2^kq^k=p\sum \limits_{k=0}^{\infty}(2q)^k$, which simplifies to $\frac{a}{1-r}$, which in our case is $p*\frac{1}{1-2q}$ because $a=1$ and $r=2q$. For $p$ to be defined, neither p nor q can equal 0.5 because that would make the denominator 0.}

\textit{We can compute $E(2^{-X})$ also using LOTUS. $E(X) = \sum \limits_{k=0}^{\infty}kpq^k$, therfore $E(2^{-X})=\sum \limits_{k=0}^{\infty}\frac{1}{2^k}pq^k$. We can simplify the Taylor Series if we pull out the constant, $p$, such that $p\sum \limits_{k=0}^{\infty}\frac{1}{2^k}q^k=p\sum \limits_{k=0}^{\infty}(q/2)^k$, which simplifies to $\frac{a}{1-r}$, which in our case is $p*\frac{1}{1-q/2}$ because $a=1$ and $r=q/2$. In this case, p is always defined in the function because q cannot be greater than 1.}
